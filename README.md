# ðŸ¤– Model Architecture: Transformer Macro Autoencoder

Transformer ê¸°ë°˜ì˜ ì˜¤í† ì¸ì½”ë”(Autoencoder) êµ¬ì¡°
- ì •ìƒ íŒ¨í„´: ëª¨ë¸ì´ ë†’ì€ ì •í™•ë„ë¡œ ë³µì›í•˜ì—¬ ìž¬êµ¬ì„± ì˜¤ì°¨ 0ì— ìˆ˜ë ´í•©ë‹ˆë‹¤.
- ì´ìƒ íŒ¨í„´(ë§¤í¬ë¡œ): ëª¨ë¸ì´ í•™ìŠµí•˜ì§€ ëª»í•œ íŒ¨í„´ì´ë¯€ë¡œ ë³µì› ëŠ¥ë ¥ì´ ë–¨ì–´ì ¸ ìž¬êµ¬ì„± ì˜¤ì°¨ê°€ ë†’ê²Œ ë°œìƒí•©ã„´ë‹¤.

- Feature Embedding : 5ì°¨ì›ì˜ ìž…ë ¥ í”¼ì²˜(x, y, dist ë“±)ë¥¼ d_model(64ì°¨ì›)ì˜ ê³ ì°¨ì› ë²¡í„°ë¡œ í™•ìž¥í•˜ì—¬ ë³µìž¡í•œ ìƒê´€ê´€ê³„ë¥¼ í•™ìŠµí•  ì¤€ë¹„ë¥¼ í•©ë‹ˆë‹¤.
- Positional Encoding : TransformerëŠ” RNNê³¼ ë‹¬ë¦¬ ìˆœì„œ ì •ë³´ê°€ ì—†ìœ¼ë¯€ë¡œ, ì‹œí€€ìŠ¤ ë‚´ ê° ìœ„ì¹˜ ì •ë³´($1^{st}, 2^{nd}, ...$)ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë²¡í„°ë¥¼ ë”í•´ì¤ë‹ˆë‹¤.
- Transformer Encoder : Multi-Head Self-Attention ë©”ì»¤ë‹ˆì¦˜ì„ í†µí•´ ì‹œí€€ìŠ¤ ì „ì²´ë¥¼ ë™ì‹œì— í›‘ìœ¼ë©°, ê³¼ê±°ì˜ ì›€ì§ìž„ì´ í˜„ìž¬ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ íŒŒì•…í•©ë‹ˆë‹¤.
- Linear Decoder : ì¸ì½”ë”ê°€ ë½‘ì•„ë‚¸ ì¶”ìƒì ì¸ íŠ¹ì§•ë“¤ì„ ë‹¤ì‹œ ì›ëž˜ì˜ 5ê°œ í”¼ì²˜ ì°¨ì›ìœ¼ë¡œ ë³µì›í•©ë‹ˆë‹¤.

Detection Logic
- Normal Patterns: The model reconstructs these with high precision, causing the reconstruction error to converge to zero.
- Anomalous Patterns (Macro): Since these are patterns the model has not encountered during training, the reconstruction capability decreases, resulting in a high reconstruction error.

- Feature Embedding: Expands the 5-dimensional input features (e.g., $x, y, dist$) into a high-dimensional vector of $d_{model}$ (64 dimensions) to prepare the model for learning complex correlations.
- Positional Encoding: Since Transformers do not inherently process sequential order like RNNs, this adds vectors that represent the positional information ($1^{st}, 2^{nd}, \dots$) within the sequence.
- Transformer Encoder: Utilizes the Multi-Head Self-Attention mechanism to scan the entire sequence simultaneously, capturing how past movements influence the present state.
- Linear Decoder: Reconstructs the abstract features extracted by the encoder back into the original 5-feature dimensions.

![Architecture Diagram](./public/Architecture.png)

---
# ì§€ì› í”„ë¡œê·¸ëž¨
- postgres
- json

# í•„ìˆ˜ íŒŒì¼
.env
```
# ê¸°ë¡ê¸°
# postgres => postgres, json => json
Recorder=json

# posgresë¥¼ ì‚¬ìš© ì‹œ ê¸°ìž…
DB_HOST=-
DB_USER=-
DB_PASSWORD=-
DB_NAME=-
DB_PORT=-

# í•„ìˆ˜ ìž…ë ¥
SEQ_LEN=100
STRIDE=50
JsonPath=./
threshold=0.7
d_model=256
num_layers=3
dropout=0.3
batch_size=64
lr=0.0005
```

# ì„¤ì¹˜ ëª©ë¡
```
pynput
torch
psycopg2-binary
SQLAlchemy
pydantic_settings
pyautogui
matplotlib
numpy
pyqtgraph 
PySide6
PyQt6
keyboard
```

ëª…ë ¹ì–´
```
pip install -r requirements.txt
```

# ì‚¬ìš© ì„¤ëª…ì„œ (Manual)
Manual.pptx

# ì˜ˆì‹œìš© ëª¨ë¸
model ê²½ë¡œ => app.models.weights
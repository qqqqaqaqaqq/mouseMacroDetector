import torch
import joblib
import numpy as np
import pandas as pd
from collections import deque
from multiprocessing import Queue, Event

import app.core.globals as g_vars
from app.models.TransformerMacroDetector import TransformerMacroAutoencoder
from app.services.indicators import indicators_generation

def inferece_plot_main(chart_queue: Queue, features, threshold, stop_event=None):
    import sys
    from app.services.RealTimeMonitor import RealTimeMonitor
    from PyQt6.QtCore import QTimer
    
    if stop_event is None:
        stop_event = Event()

    monitor = RealTimeMonitor(features, threshold)
    
    def update():
        if stop_event.is_set():
            timer.stop()
            monitor.app.quit()
            return

        try:
            while not chart_queue.empty():
                data = chart_queue.get_nowait()
                # data: (tensor_np, error, current_threshold)
                if len(data) == 3:
                    monitor.update_view(data[0], data[1], data[2])
                else:
                    monitor.update_view(data[0], data[1], threshold)
        except (EOFError, BrokenPipeError, ConnectionResetError):
            timer.stop()
            monitor.app.quit()
        except Exception:
            pass
                
    timer = QTimer()
    timer.timeout.connect(update)
    timer.start(16)
    sys.exit(monitor.app.exec())

class MacroDetector:
    def __init__(self, model_path: str, seq_len=g_vars.SEQ_LEN, threshold=None, device=None, chart_Show=True, stop_event=None):
        self.seq_len = seq_len
        self.base_threshold = threshold if threshold is not None else g_vars.threshold
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")

        # [ë³€ê²½] ìµœê·¼ 100ê°œ ë°ì´í„° í¬ì¸íŠ¸ë§Œ ìœ ì§€ (ì—°ì‚° íš¨ìœ¨í™”)
        self.buffer = deque(maxlen=100) 
        
        # ë…¸ì´ì¦ˆ ë°©ì§€ë¥¼ ìœ„í•´ ìµœê·¼ 3~5ê°œ ì—ëŸ¬ì˜ í‰ê· ë§Œ ì‚¬ìš© (ìˆœê°„ì ì¸ íŠ ë°©ì§€)
        self.smooth_error_buf = deque(maxlen=5) 
        
        self.stop_event = stop_event or Event()
        self.chart_Show = chart_Show
        self.plot_proc = None

        # ===== ëª¨ë¸ ì´ˆê¸°í™” =====
        self.model = TransformerMacroAutoencoder(
            input_size=len(g_vars.FEATURES),
            d_model=g_vars.d_model,
            nhead=g_vars.n_head,
            num_layers=g_vars.num_layers,
            dim_feedforward=g_vars.dim_feedforward,
            dropout=g_vars.dropout
        ).to(self.device)

        self.model.load_state_dict(torch.load(model_path, map_location=self.device, weights_only=True))
        self.model.eval()

        self.scaler = joblib.load(g_vars.scaler_path)

    def push(self, data: dict):
        self.buffer.append((data.get('x'), data.get('y'), data.get('timestamp'), data.get('deltatime')))
        
        # ìµœì†Œ seq_lenì€ ì±„ì›Œì ¸ì•¼ ë¶„ì„ ì‹œì‘
        if len(self.buffer) < self.seq_len:
            return None
        return self._infer()

    def start_plot_process(self):
        """ì‹¤ì‹œê°„ ì°¨íŠ¸ í”„ë¡œì„¸ìŠ¤ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤."""
        if not self.chart_Show or (self.plot_proc and self.plot_proc.is_alive()):
            return

        from multiprocessing import Process
        # inferece_plot_mainëŠ” íŒŒì¼ ìƒë‹¨ì— ì •ì˜ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
        self.plot_proc = Process(
            target=inferece_plot_main, 
            args=(g_vars.CHART_DATA, g_vars.FEATURES, self.base_threshold, self.stop_event),
            daemon=False
        )
        self.plot_proc.start()

    def _infer(self):
        # 1. ìµœê·¼ 100ê°œ ë°ì´í„°ë¡œ í”¼ì²˜ ìƒì„±
        df = pd.DataFrame(list(self.buffer), columns=["x", "y", "timestamp", "deltatime"])
        df = indicators_generation(df)

        # 2. ëª¨ë¸ ì…ë ¥ìš© ë§ˆì§€ë§‰ seq_len ì¶”ì¶œ
        df_features = df[g_vars.FEATURES].tail(self.seq_len).copy()
        
        if g_vars.CLIP_BOUNDS:
            for col, b in g_vars.CLIP_BOUNDS.items():
                if col in df_features.columns:
                    df_features[col] = df_features[col].clip(lower=b['min'], upper=b['max'])

        try:
            X_scaled = self.scaler.transform(df_features.values)
            X_tensor = torch.tensor(X_scaled, dtype=torch.float32).unsqueeze(0).to(self.device)
            
            with torch.no_grad():
                output = self.model(X_tensor)
                # ì¬êµ¬ì„± ì—ëŸ¬ (Reconstruction Error)
                recon_error = torch.mean((output - X_tensor)**2).item()
        except Exception as e:
            print(f"âŒ Inference Error: {e}")
            return None

        # 3. ì—ëŸ¬ ìŠ¤ë¬´ë”© (ë„ˆë¬´ ë¯¼ê°í•˜ê²Œ ë°˜ì‘í•˜ì§€ ì•Šë„ë¡ ìµœê·¼ 5ê°œ í‰ê· )
        self.smooth_error_buf.append(recon_error)
        avg_error = np.mean(self.smooth_error_buf)

        # 4. [ë³€ê²½] ë‹¨ìˆœ Threshold íŒì • ë¡œì§
        # í‰ê·  ì—ëŸ¬ê°€ ì„¤ì •í•œ ì„ê³„ê°’ì„ ë„˜ìœ¼ë©´ ë°”ë¡œ ë§¤í¬ë¡œ(False) íŒì •
        is_human = avg_error < self.base_threshold * 1.05
        
        # ì‹œê°ì ì¸ í™•ë¥  í‘œê¸° (ë‹¨ìˆœíˆ ì—ëŸ¬/ì„ê³„ê°’ ë¹„ìœ¨ë¡œ í‘œì‹œ)
        macro_score = min(100.0, round((avg_error / self.base_threshold) * 50, 2))
        if not is_human:
            # ì„ê³„ê°’ì„ ë„˜ëŠ” ìˆœê°„ 50~100 ì‚¬ì´ë¡œ í‘œê¸°
            macro_score = min(100.0, 50.0 + (avg_error - self.base_threshold) * 100)

        # 5. ëª¨ë‹ˆí„°ë§ ë°ì´í„° ì „ì†¡
        if g_vars.CHART_DATA is not None:
            try:
                g_vars.CHART_DATA.put_nowait((X_tensor.cpu().numpy(), avg_error, self.base_threshold))
            except: pass

        return {
            "is_human": is_human,
            "macro_probability": f"{'ğŸš¨ MACRO' if not is_human else 'ğŸ™‚ HUMAN'}",
            "prob_value": macro_score,
            "raw_error": round(avg_error, 5),
            "threshold": self.base_threshold
        }
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import pandas as pd
import time
import numpy as np
import json


from sklearn.model_selection import train_test_split
from sklearn.preprocessing import RobustScaler

import app.core.globals as g_vars
import joblib
# ëª¨ë¸
from app.models.MousePoint import MousePoint
from app.models.TransformerMacroDetector import TransformerMacroAutoencoder, MacroDataset

import app.repostitories.DBController as DBController
import app.repostitories.JsonController as JsonController

from app.services.indicators import indicators_generation
from multiprocessing import Queue

from app.utilites.make_df_from_points import make_df_from_points
from app.utilites.points_to_features import points_to_features
from app.utilites.save_confing import update_parameters

def train_plot_main(train_queue: Queue):
    import sys
    from app.services.RealTimeMonitor import TrainMonitor
    from PyQt6.QtCore import QTimer

    monitor = TrainMonitor(window_size=1000)

    def update():
        while not train_queue.empty():
            epoch, train_loss, val_loss = train_queue.get_nowait()
            monitor.update_view(epoch, train_loss, val_loss)

    timer = QTimer()
    timer.timeout.connect(update)
    timer.start(100)

    sys.exit(monitor.app.exec())


class TrainMode():
    def __init__(self, stop_event=None, log_queue:Queue=None):
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'

        self.stop_event=stop_event
        self.seq_len=g_vars.SEQ_LEN
        self.log_queue:Queue=log_queue

        self.plot_proc = None


    def start_plot_process(self):
        if self.plot_proc is not None and self.plot_proc.is_alive():
            return

        from multiprocessing import Process
        self.plot_proc = Process(
            target=train_plot_main,
            args=(g_vars.TRAIN_DATA,),
            daemon=False
        )
        self.plot_proc.start()

    # train
    def train_start(self, train_dataset, val_dataset, batch_size=g_vars.batch_size, epochs=2000, lr=g_vars.lr,
                    device=None, model=None, stop_event=None, patience=20, log_queue=None, save_path="best_model.pth"):
        try:
            log_queue.put(f"lr : {lr}, batch_size : {batch_size}")
            self.start_plot_process()

            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
            val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

            criterion = nn.MSELoss()
            optimizer = torch.optim.AdamW(
                model.parameters(), 
                lr=lr,
                weight_decay=g_vars.weight_decay
            )

            best_val_loss = float('inf')
            epochs_no_improve = 0
            # 5% ê°œì„ ì„ ê¸°ì¤€ìœ¼ë¡œ ì„¤ì • (í•„ìš”ì— ë”°ë¼ 0.05 ~ 0.20 ì¡°ì ˆ)
            min_improvement_factor = 0.05 
            recommended_base_threshold = 0.0

            for epoch in range(epochs):
                if stop_event.is_set():
                    self.train_stop_event(log_queue=log_queue)
                    break

                # ===== Train Phase =====
                model.train()
                total_train_loss = 0
                for batch_x in train_loader:
                    batch_x = batch_x.to(device)
                    optimizer.zero_grad()
                    outputs = model(batch_x)
                    loss = criterion(outputs, batch_x)
                    loss.backward()
                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                    optimizer.step()
                    total_train_loss += loss.item() * batch_x.size(0)

                avg_train_loss = total_train_loss / len(train_dataset)

                # ===== Validation Phase =====
                model.eval()
                total_val_loss = 0
                all_val_errors = [] 

                with torch.no_grad():
                    for batch_x in val_loader:
                        batch_x = batch_x.to(device)
                        outputs = model(batch_x)
                        
                        loss = criterion(outputs, batch_x)
                        total_val_loss += loss.item() * batch_x.size(0)
                        
                        # MSE ê¸°ë°˜ ì„ê³„ì¹˜ ê³„ì‚°ìš© ë°ì´í„° ìˆ˜ì§‘
                        sample_errors = torch.mean((outputs - batch_x)**2, dim=(1, 2)) 
                        all_val_errors.extend(sample_errors.cpu().numpy())

                avg_val_loss = total_val_loss / len(val_dataset)
                current_epoch_threshold = np.percentile(all_val_errors, 99.7)

                # ===== Logging & Early Stopping Logic =====
                status_msg = f"Epoch {epoch+1}/{epochs} | Train: {avg_train_loss:.6f} | Val: {avg_val_loss:.6f} | Est.Thresh: {current_epoch_threshold:.6f}"
                if log_queue: log_queue.put(status_msg)

                # ì‹œê°í™” ë°ì´í„° í ì „ë‹¬
                if g_vars.TRAIN_DATA is not None:
                   g_vars.TRAIN_DATA.put((epoch + 1, float(avg_train_loss), float(avg_val_loss)))

                # ê°œì„ ìœ¨ ê³„ì‚°
                improvement = 0
                if best_val_loss != float('inf'):
                    improvement = (best_val_loss - avg_val_loss) / best_val_loss

                # [í•µì‹¬ ë¡œì§] ì²« ì—í¬í¬ì´ê±°ë‚˜, ê°œì„ ìœ¨ì´ ê¸°ì¤€ì¹˜ ì´ìƒì¼ ë•Œ
                if best_val_loss == float('inf') or improvement >= min_improvement_factor:
                    is_initial = (best_val_loss == float('inf'))
                    best_val_loss = avg_val_loss
                    recommended_base_threshold = current_epoch_threshold
                    epochs_no_improve = 0
                    torch.save(model.state_dict(), save_path)
                    
                    if log_queue:
                        msg = "First Best" if is_initial else f"Improvement {improvement*100:.1f}%"
                        log_queue.put(f" >> [Model Saved] {msg} | Loss: {best_val_loss:.6f}")
                else:
                    epochs_no_improve += 1
                    if log_queue:
                        log_queue.put(f" >> [No Significant Improve] Count: {epochs_no_improve}/{patience} (Improv: {improvement*100:.1f}%)")
                    
                    if epochs_no_improve >= patience:
                        if log_queue: log_queue.put(f"Early Stopping: {min_improvement_factor*100}% ì´ìƒì˜ ê°œì„ ì´ {patience} epoch ë™ì•ˆ ì—†ìŒ.")
                        if stop_event: stop_event.set()
                        break

            # ìµœì¢… íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
            final_msg = f"í•™ìŠµ ì¢…ë£Œ. ìµœì  Val Loss: {best_val_loss:.6f} | ì¶”ì²œ Base Threshold: {recommended_base_threshold:.6f}"
            if log_queue: log_queue.put(final_msg)
            
            g_vars.threshold = recommended_base_threshold 
            update_parameters({"THRES" : recommended_base_threshold})
            
            with g_vars.lock:
                g_vars.GLOBAL_CHANGE = True

        finally:
            self.train_stop_event(log_queue=log_queue)
            if 'model' in locals() and model is not None:
                model.to('cpu')
            import gc
            gc.collect()
            torch.cuda.empty_cache()
        
    def main(self):
        self.log_queue.put(f"device : {self.device} | SEQ_LEN : {g_vars.SEQ_LEN} | STRIDE : {g_vars.STRIDE}")

        # ===== ë°ì´í„° ì½ê¸° =====
        if g_vars.Recorder == "postgres":
            user_all: list[MousePoint] = DBController.read(True, log_queue=self.log_queue)
            is_dict = False
        elif g_vars.Recorder == "json":
            user_all: list[dict] = JsonController.read(True, log_queue=self.log_queue)
            is_dict = True

        self.log_queue.put(f"user_all length : {len(user_all)}")

        user_df_chunk = make_df_from_points(user_all, is_dict=is_dict)

        user_df_chunk= user_df_chunk.sort_values('timestamp').reset_index(drop=True)

        # ===== Feature ê³„ì‚° =====
        setting_user_df_chunk: pd.DataFrame = indicators_generation(user_df_chunk)
        
        # ===== Feature í•„í„° =====
        setting_user_df_chunk = setting_user_df_chunk[g_vars.FEATURES].copy()

        clip_bounds_dict = {}
        for col in g_vars.FEATURES:
            # 1. ì¼ë‹¨ ì½´íƒ€ì¼ë¡œ ë²”ìœ„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
            lower = float(setting_user_df_chunk[col].quantile(0.05))
            upper = float(setting_user_df_chunk[col].quantile(0.95))
            
            # 2. [í•µì‹¬ ìˆ˜ì •] ì†ë„ì™€ ê´€ë ¨ëœ ì§€í‘œë“¤ì€ 'ì›€ì§ì„ ì—†ìŒ(0)'ì„ í—ˆìš©í•´ì•¼ í•©ë‹ˆë‹¤.
            # í•™ìŠµ ë°ì´í„°ì— ì›€ì§ì„ë§Œ ìˆë”ë¼ë„, ì¶”ë¡  ì‹œ ì •ì§€ ìƒíƒœë¥¼ ìœ„í•´ minì„ 0ìœ¼ë¡œ ê°•ì œí•©ë‹ˆë‹¤.
            if col in ['speed', 'speed_var', 'jerk_std', 'straightness']:
                # straightnessëŠ” ë³´í†µ 1ì´ ìµœì†Œì§€ë§Œ ì•ˆì „í•˜ê²Œ 0ì´ë‚˜ ë°ì´í„° ìµœì†Œê°’ ì¤‘ ì‘ì€ ìª½ ì„ íƒ
                lower = 0.0 
            
            # 3. ê° ì§€í‘œë³„ íŠ¹ì„±ì— ë”°ë¥¸ í•˜í•œì„  ë³´ì • (í•„ìš”ì‹œ)
            # turn, acc ë“±ì€ ìŒìˆ˜ê°€ ê°€ëŠ¥í•˜ë¯€ë¡œ ì½´íƒ€ì¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            
            clip_bounds_dict[col] = {"min": lower, "max": upper}
            setting_user_df_chunk[col] = setting_user_df_chunk[col].clip(lower, upper)

            
        
        # ì „ì—­ ë³€ìˆ˜ ì—…ë°ì´íŠ¸
        g_vars.CLIP_BOUNDS = clip_bounds_dict
        update_parameters({"CLIP_BOUNDS" : clip_bounds_dict})
        
        print(f"{json.dumps(clip_bounds_dict, indent=2)} Save")

        print(f"setting_user_df_chunk : {setting_user_df_chunk}")


        if len(setting_user_df_chunk) < g_vars.SEQ_LEN:
            self.log_queue.put("ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return

        # ===== Train/Val split =====
        user_train_df: pd.DataFrame
        user_val_df: pd.DataFrame

        user_train_df, user_val_df = train_test_split(setting_user_df_chunk, test_size=0.2, shuffle=False)

        # ===== Sequence =====q
        user_train_seq, user_train_pass_seq = points_to_features(df_chunk=user_train_df, seq_len=g_vars.SEQ_LEN, stride=g_vars.STRIDE, log_queue=self.log_queue)

        user_val_seq, user_val_pass_seq = points_to_features(df_chunk=user_val_df, seq_len=g_vars.SEQ_LEN, stride=g_vars.STRIDE, log_queue=self.log_queue)

        self.log_queue.put(f"Length => user_train_seq : {user_train_pass_seq}")
        self.log_queue.put(f"Length => user_val_pass_seq : {user_val_pass_seq}")

        X_train = user_train_seq
        X_val = user_val_seq

        # ìŠ¤ì¼€ì¼ë§ ì ìš©
        n_train, seq_len, n_features = X_train.shape
        n_val = X_val.shape[0]

        scaler = RobustScaler()
        
        # [Train ìŠ¤ì¼€ì¼ë§] 2Dë¡œ í¼ì³ì„œ í•™ìŠµ(fit) í›„ ë³€í™˜(transform)
        X_train_reshaped = X_train.reshape(-1, n_features)
        X_train_scaled = scaler.fit_transform(X_train_reshaped)
        X_train = X_train_scaled.reshape(n_train, seq_len, n_features)

        # [Val ìŠ¤ì¼€ì¼ë§] Train ê¸°ì¤€ ìŠ¤ì¼€ì¼ëŸ¬ë¡œ ë³€í™˜ë§Œ ìˆ˜í–‰ (ì¤‘ìš”: fit ì•ˆí•¨)
        X_val_reshaped = X_val.reshape(-1, n_features)
        X_val_scaled = scaler.transform(X_val_reshaped)
        X_val = X_val_scaled.reshape(n_val, seq_len, n_features)
        joblib.dump(scaler, g_vars.scaler_path)
        
        train_dataset = MacroDataset(X_train)
        val_dataset   = MacroDataset(X_val)
        
        print(f"Total samples: {len(train_dataset)}")

        # 2. ì²« ë²ˆì§¸ ìƒ˜í”Œ ê°€ì ¸ì˜¤ê¸°
        sample_x = train_dataset[0]

        print("--- First Sample Data ---")
        print(f"Input Shape (seq_len, features): {sample_x.shape}")
        print("--- Raw Input (First 5 steps) ---")
        print(sample_x[:5])

        model = TransformerMacroAutoencoder(
            input_size=len(g_vars.FEATURES),
            d_model=g_vars.d_model,
            nhead=g_vars.n_head,
            num_layers=g_vars.num_layers,
            dim_feedforward=g_vars.dim_feedforward,
            dropout=g_vars.dropout
        ).to(self.device)

        timeinterval = 7

        while timeinterval != 0:
            if self.stop_event.is_set():
                self.train_stop_event(log_queue=self.log_queue)
                return

            timeinterval -= 1
            self.log_queue.put(f"train ì‹œì‘ê¹Œì§€ count : {timeinterval}")

            time.sleep(1)

        self.train_start(
            train_dataset=train_dataset,
            val_dataset=val_dataset, 
            batch_size=g_vars.batch_size, 
            lr=g_vars.lr,
            epochs=g_vars.epoch,
            device=self.device, 
            model=model, 
            stop_event=self.stop_event, 
            patience=g_vars.patience, 
            log_queue=self.log_queue, 
            save_path=g_vars.save_path
        )

    def train_stop_event(self, log_queue: Queue = None):
        # 1. ì‹œê°í™” í”„ë¡œì„¸ìŠ¤ëŠ” ê±´ë“œë¦¬ì§€ ì•ŠìŒ (ì‚´ë ¤ë‘ )
        if log_queue: 
            log_queue.put("âœ… í•™ìŠµ ë£¨í”„ ì¢…ë£Œ (ê·¸ë˜í”„ëŠ” ìœ ì§€ë©ë‹ˆë‹¤)")

        # 2. GPU ë©”ëª¨ë¦¬ ì •ë¦¬ (ì´ê²ƒë§Œ í•´ë„ ë¦¬ì†ŒìŠ¤ í™•ë³´ì— í° ë„ì›€ì´ ë©ë‹ˆë‹¤)
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        if log_queue: 
            log_queue.put("ğŸ§¹ GPU Cache Cleared")

        with g_vars.lock:
            g_vars.GLOBAL_CHANGE = True